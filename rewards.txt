Folder Structure
--------------------------------------------------
rewards/
    base_reward.py
    composite_reward.py
    constant_reward.py
    default_reward.py
    rightward_reward.py
    soft_landing_reward.py
    __init__.py


File Contents
--------------------------------------------------


lander_learner/rewards\base_reward.py
File type: .py
from abc import ABC, abstractmethod
import operator


class BaseReward(ABC):
    def __init__(self, **kwargs):
        """
        Base class for all reward objects.
        Subclasses should override get_reward().

        Operator overloading is enabled:
          - r1 + r2 returns a CompositeReward computing r1.get_reward() + r2.get_reward()
          - r1 - r2, r1 * r2, r1 / r2, etc., are similarly supported.
          - Scalars are automatically wrapped in ConstantReward.
        """
        pass

    @abstractmethod
    def get_reward(self, env, done: bool) -> float:
        """
        Compute and return the reward given the environment state and termination flag.
        """
        pass

    # Operator overloads:
    def __add__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(self, operator.add, other)

    def __radd__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(other, operator.add, self)

    def __sub__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(self, operator.sub, other)

    def __rsub__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(other, operator.sub, self)

    def __mul__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(self, operator.mul, other)

    def __rmul__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(other, operator.mul, self)

    def __truediv__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(self, operator.truediv, other)

    def __rtruediv__(self, other):
        from .composite_reward import CompositeReward
        return CompositeReward(other, operator.truediv, self)


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/rewards\composite_reward.py
File type: .py
from .base_reward import BaseReward
from .constant_reward import ConstantReward


class CompositeReward(BaseReward):
    """
    CompositeReward applies a binary operation on two reward operands.

    The operands can be either:
      - Instances of BaseReward
      - Scalars (automatically wrapped into ConstantReward)

    Parameters:
        left (BaseReward or scalar): The left operand.
        op (callable): A binary operator function (e.g. operator.add).
        right (BaseReward or scalar): The right operand.
    """
    def __init__(self, left, op, right):
        self.left = left if isinstance(left, BaseReward) else ConstantReward(left)
        self.right = right if isinstance(right, BaseReward) else ConstantReward(right)
        self.op = op

    def get_reward(self, env, done: bool) -> float:
        left_value = self.left.get_reward(env, done)
        right_value = self.right.get_reward(env, done)
        return self.op(left_value, right_value)


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/rewards\constant_reward.py
File type: .py
from .base_reward import BaseReward


class ConstantReward(BaseReward):
    """
    A constant reward that always returns a fixed scalar value.

    Parameters:
        value (float): The constant reward value.
    """
    def __init__(self, value: float):
        self.value = value

    def get_reward(self, env, done: bool) -> float:
        return self.value


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/rewards\default_reward.py
File type: .py
import numpy as np
from lander_learner.rewards.base_reward import BaseReward
from lander_learner.utils.config import Config, RL_Config


class DefaultReward(BaseReward):
    def __init__(self, **kwargs):
        """
        Initialize DefaultReward with configurable parameters.

        Possible keyword arguments:
            x_velocity_factor (float): Factor for rewarding rightward velocity.
                                        Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["x_velocity_factor"]
            angle_penalty_factor (float): Factor for penalizing deviation from π/2.
                                          Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["angle_penalty_factor"]
            collision_penalty (float): Penalty per time step when a collision is detected.
                                       Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["collision_penalty"]
            crash_penalty_multiplier (float): Multiplier for penalty based on collision impulse on termination.
                                              Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["
                                              crash_penalty_multiplier"]
        """
        defaults = RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS
        self.x_velocity_factor = kwargs.get("x_velocity_factor", defaults["x_velocity_factor"])
        self.angle_penalty_factor = kwargs.get("angle_penalty_factor", defaults["angle_penalty_factor"])
        self.collision_penalty = kwargs.get("collision_penalty", defaults["collision_penalty"])
        self.crash_penalty_multiplier = kwargs.get("crash_penalty_multiplier", defaults["crash_penalty_multiplier"])

    def get_reward(self, env, done: bool) -> float:
        reward = 0.0
        if done:
            if env.crash_state:
                reward -= env.collision_impulse * self.crash_penalty_multiplier
            return float(reward)

        # Reward rightward motion and penalize deviation from π/2.
        x_velocity = env.lander_velocity[0]
        angle_error = abs((env.lander_angle - np.pi / 2) % np.pi)
        reward += (
            x_velocity * self.x_velocity_factor - angle_error * self.angle_penalty_factor
        ) * Config.FRAME_TIME_STEP

        if env.collision_state:
            reward -= self.collision_penalty * Config.FRAME_TIME_STEP

        return float(reward)


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/rewards\rightward_reward.py
File type: .py
import numpy as np
from lander_learner.rewards.base_reward import BaseReward
from lander_learner.utils.config import Config, RL_Config
import logging

logger = logging.getLogger(__name__)


class RightwardReward(BaseReward):
    def __init__(self, **kwargs):
        """
        Initialize DefaultReward with configurable parameters.

        Possible keyword arguments:
            x_velocity_factor (float): Factor for rewarding rightward velocity.
                                        Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["x_velocity_factor"]
            angle_penalty_factor (float): Factor for penalizing deviation from π/2.
                                          Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["angle_penalty_factor"]
            collision_penalty (float): Penalty per time step when a collision is detected.
                                       Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["collision_penalty"]
            crash_penalty_multiplier (float): Multiplier for penalty based on collision impulse on termination.
                                              Default: RL_Config.DEFAULT_DEFAULT_REWARD_PARAMS["
                                              crash_penalty_multiplier"]
        """
        defaults = RL_Config.DEFAULT_RIGHTWARD_REWARD_PARAMS
        recognised_params = (
            "x_velocity_factor",
            "angle_penalty_factor",
            "collision_penalty",
            "crash_penalty_multiplier"
        )
        for param in recognised_params:
            try:
                setattr(self, param, float(kwargs.get(param, defaults[param])))
            except (ValueError, TypeError):
                raise logger.fatal(f"{param} must be a float", exc_info=True)
        extra_params = set(kwargs) - set(recognised_params)
        for param in extra_params:
            logger.warning(f"Unrecognized parameter: {param}")

    def get_reward(self, env, done: bool) -> float:
        reward = 0.0

        # Penalize crash
        if done:
            if env.crash_state:
                reward -= env.collision_impulse * self.crash_penalty_multiplier
            logger.debug(f"Final reward: {reward:.2f}")
            return float(reward)

        # Reward rightward motion and heading angle towards right
        x_velocity = env.lander_velocity[0]
        angle_error = abs((env.lander_angle - np.pi / 2) % np.pi)
        reward += (
            x_velocity * self.x_velocity_factor - angle_error * self.angle_penalty_factor
        ) * Config.FRAME_TIME_STEP

        # Penalize collision
        if env.collision_state:
            reward -= self.collision_penalty * Config.FRAME_TIME_STEP

        return float(reward)


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/rewards\soft_landing_reward.py
File type: .py
import numpy as np
from lander_learner.rewards.base_reward import BaseReward
from lander_learner.utils.config import Config, RL_Config
import logging

logger = logging.getLogger(__name__)


class SoftLandingReward(BaseReward):
    def __init__(self, **kwargs):
        """
        Initialize SoftLandingReward with configurable parameters.

        Possible keyword arguments:
            on_target_touch_down_bonus (float): Bonus reward for a soft landing within the target zone.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["on_target_touch_down_bonus"]
            off_target_touch_down_penalty (float): Penalty for touching down off target.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["off_target_touch_down_penalty"]
            on_target_idle_bonus (float): Bonus reward for idling within the target zone.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["on_target_idle_bonus"]
            off_target_idle_penalty (float): Penalty for idling off target.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["off_target_idle_penalty"]
            crash_penalty_multiplier (float): Multiplier for penalty based on collision impulse on termination.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["crash_penalty_multiplier"]
            time_penalty_factor (float): Factor for penalizing time taken.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["time_penalty_factor"]
            travel_reward_factor (float): Factor for rewarding travel towards the target.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["travel_reward_factor"]
            near_target_off_angle_penalty (float): Penalty for being off-angle near the target.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["near_target_off_angle_penalty"]
            near_target_high_velocity_penalty (float): Penalty for high velocity near the target.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["near_target_high_velocity_penalty"]
            near_target_unit_dist (float): Unit distance for near target calculations.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["near_target_unit_dist"]
            near_target_max_multiplier (float): Maximum multiplier for near target calculations.
                Default: RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS["near_target_max_multiplier"]
        """
        defaults = RL_Config.DEFAULT_SOFT_LANDING_REWARD_PARAMS
        recognized_params = (
            "on_target_touch_down_bonus",
            "off_target_touch_down_penalty",
            "on_target_idle_bonus",
            "off_target_idle_penalty",
            "crash_penalty_multiplier",
            "time_penalty_factor",
            "travel_reward_factor",
            "near_target_off_angle_penalty",
            "near_target_high_velocity_penalty",
            "near_target_high_velocity_cut_off",
            "near_target_unit_dist",
            "near_target_max_multiplier",
            "near_target_passive_bonus"
        )
        for param in recognized_params:
            try:
                setattr(self, param, float(kwargs.get(param, defaults[param])))
            except (ValueError, TypeError):
                raise logger.fatal(f"{param} must be a float", exc_info=True)
        extra_params = set(kwargs) - set(recognized_params)
        for param in extra_params:
            logger.warning(f"Unrecognized parameter: {param}")

    def get_reward(self, env, done: bool) -> float:
        reward = 0.0

        vector_to_target = env.target_position - env.lander_position
        distance_to_target = np.linalg.norm(vector_to_target)

        # Penalize crash and reward soft landing in target zone
        if done:
            if env.crash_state:
                reward -= (self.crash_penalty_multiplier * env.collision_impulse
                           + self.time_penalty_factor * (Config.MAX_EPISODE_DURATION - env.elapsed_time))
            elif env.idle_state:
                reward += (
                    self.on_target_idle_bonus
                    - (self.on_target_idle_bonus + self.off_target_idle_penalty)
                    * np.clip(distance_to_target / env.target_zone_width, 0.0, 1.0)
                    ) * (Config.MAX_EPISODE_DURATION - env.elapsed_time)
                # in_target = (
                #     env.target_position[0] - env.target_zone_width / 2
                #     <= env.lander_position[0]
                #     <= env.target_position[0] + env.target_zone_width / 2
                #     and env.target_position[1] - env.target_zone_height / 2
                #     <= env.lander_position[1]
                #     <= env.target_position[1] + env.target_zone_height / 2
                # )
                # if in_target:
                #     reward += self.on_target_idle_bonus * (Config.MAX_EPISODE_DURATION - env.elapsed_time)
                # else:
                #     reward -= self.off_target_idle_penalty * (Config.MAX_EPISODE_DURATION - env.elapsed_time)
            elif env.time_limit_reached:
                pass
            else:
                logger.warning("Unrecognised termination condition. No reward assigned.")
            logger.debug(f"Final reward: {reward:.2f}")
            return float(reward)

        # Reward travel toward target position
        reward += (
            self.travel_reward_factor
            * np.dot(env.lander_velocity, vector_to_target)
            / distance_to_target
            - self.time_penalty_factor
            ) * Config.FRAME_TIME_STEP

        # Encourage being upright and moving slowly near the target
        angle_penalty = abs(((env.lander_angle + np.pi) % (2 * np.pi)) - np.pi) / np.pi
        velocity_penalty = np.linalg.norm(np.clip(env.lander_velocity, 1.0, None) - 1.0)
        reward -= (
            (self.near_target_off_angle_penalty * angle_penalty
             + self.near_target_high_velocity_penalty * velocity_penalty
             + self.near_target_passive_bonus)
            * (self.near_target_unit_dist
               / np.clip(distance_to_target, self.near_target_unit_dist / self.near_target_max_multiplier, np.inf))
            * Config.FRAME_TIME_STEP
        )

        # Penalize collision
        if env.collision_state:
            reward += (
                self.on_target_touch_down_bonus
                - (self.on_target_touch_down_bonus + self.off_target_touch_down_penalty)
                * np.clip(distance_to_target / env.target_zone_width, 0.0, 1.0)
                ) * Config.FRAME_TIME_STEP

        return float(reward)


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/rewards\__init__.py
File type: .py
from .base_reward import BaseReward
from .default_reward import DefaultReward
from .rightward_reward import RightwardReward
from .soft_landing_reward import SoftLandingReward


def get_reward_class(name: str, **kwargs) -> BaseReward:
    """
    Factory method to create a reward instance.

    Keyword arguments are passed to the reward constructor and used to override the default parameters.

    If an unknown name is provided, DefaultReward is used.
    """
    mapping = {
        "default": DefaultReward,
        "rightward": RightwardReward,
        "soft_landing": SoftLandingReward,
    }
    reward_cls = mapping.get(name, DefaultReward)
    return reward_cls(**kwargs)


--------------------------------------------------
File End
--------------------------------------------------
