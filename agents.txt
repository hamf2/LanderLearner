Folder Structure
--------------------------------------------------
agents/
    base_agent.py
    human_agent.py
    ppo_agent.py
    sac_agent.py
    __init__.py
    

File Contents
--------------------------------------------------


lander_learner/agents\base_agent.py
File type: .py
class BaseAgent:
    """
    Base class for RL agents.
    Subclasses should override the train, get_action, save_model, and load_model methods.
    """

    def __init__(self, env, deterministic=True):
        """
        env: The Gym environment.
        deterministic: Whether the agent should act deterministically.
        """
        self.env = env
        self.deterministic = deterministic
        self.model = None

    def train(self, timesteps):
        """
        Train the agent for a specified number of timesteps.
        """
        raise NotImplementedError("Subclasses must implement train()")

    def get_action(self, observation):
        """
        Given an observation, return an action.
        """
        raise NotImplementedError("Subclasses must implement get_action()")

    def save_model(self, path):
        """
        Save the model to the specified path.
        """
        raise NotImplementedError("Subclasses must implement save_model()")

    def load_model(self, path):
        """
        Load the model from the specified path.
        """
        raise NotImplementedError("Subclasses must implement load_model()")


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/agents\human_agent.py
File type: .py
import numpy as np
import pygame
from lander_learner.agents.base_agent import BaseAgent


class HumanAgent(BaseAgent):
    """
    Provides a simple interface to read user input for thruster control.
    Integrated with the pygame event loop in `gui.py`.
    """

    def __init__(self, env):
        super().__init__(env, deterministic=True)
        self.state_flags = {"left_thruster": False, "right_thruster": False}

    def get_action(self, observation):
        """
        Return a 2D action vector: [left_thruster, right_thruster].
        The state_flags are set via keyboard events.
        """
        return np.array(
            [-1.0 + 2.0 * self.state_flags["left_thruster"], -1.0 + 2.0 * self.state_flags["right_thruster"]],
            dtype=np.float32,
        )

    def handle_key_event(self, event: pygame.event.Event):
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_LEFT:
                self.state_flags["left_thruster"] = True
            elif event.key == pygame.K_RIGHT:
                self.state_flags["right_thruster"] = True
        elif event.type == pygame.KEYUP:
            if event.key == pygame.K_LEFT:
                self.state_flags["left_thruster"] = False
            elif event.key == pygame.K_RIGHT:
                self.state_flags["right_thruster"] = False

    def train(self, timesteps):
        # Human agent learns independently.
        pass

    def save_model(self, path):
        # Nothing to save for the human agent.
        pass

    def load_model(self, path):
        # Nothing to load for the human agent.
        pass


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/agents\ppo_agent.py
File type: .py
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from lander_learner.utils.helpers import adjust_save_path, adjust_load_path
from lander_learner.utils.config import RL_Config
from lander_learner.agents.base_agent import BaseAgent
from lander_learner.agents import default_callback


class PPOAgent(BaseAgent):
    """
    An RL agent that uses Proximal Policy Optimization (PPO).
    """

    def __init__(self, env, deterministic=True, **kwargs):
        # Only run check_env for non-vectorized environments.
        if not (isinstance(env, SubprocVecEnv) or isinstance(env, DummyVecEnv)):
            check_env(env, warn=True)
        super().__init__(env, deterministic)
        self.model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            tensorboard_log=str(RL_Config.DEFAULT_LOGGING_DIR / "lander_tensorboard"),
            **kwargs  # Extra arguments passed to PPO, if any
        )

    def train(self, timesteps=RL_Config.CHECKPOINT_FREQ, callback=None):
        if callback is None:
            callback = default_callback(checkpoint_freq=timesteps, model_type="ppo")
        self.model.learn(
            total_timesteps=timesteps,
            tb_log_name="PPO_" + datetime.now().strftime("%Y%m%d-%H%M%S"),
            callback=callback
        )

    def get_action(self, observation):
        action, _states = self.model.predict(observation, deterministic=self.deterministic)
        return action

    def save_model(self, path):
        path = adjust_save_path(path, model_type="ppo")
        self.model.save(path)

    def load_model(self, path):
        path = adjust_load_path(path, model_type="ppo")
        self.model = PPO.load(path, env=self.env)


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/agents\sac_agent.py
File type: .py
from datetime import datetime
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from lander_learner.utils.helpers import adjust_save_path, adjust_load_path
from lander_learner.utils.config import RL_Config
from lander_learner.agents.base_agent import BaseAgent
from lander_learner.agents import default_callback


class SACAgent(BaseAgent):
    """
    An RL agent that uses Soft Actor-Critic (SAC) with GPU acceleration where available.
    """

    def __init__(self, env, deterministic=True, device="auto", **kwargs):
        # Only run check_env for non-vectorized environments.
        if not (isinstance(env, SubprocVecEnv) or isinstance(env, DummyVecEnv)):
            check_env(env, warn=True)
        super().__init__(env, deterministic)
        self.device = device
        self.model = SAC(
            "MlpPolicy",
            env,
            verbose=1,
            tensorboard_log=str(RL_Config.DEFAULT_LOGGING_DIR / "lander_tensorboard"),
            device=self.device,  # Uses GPU if available: "cuda" or "auto"
            **kwargs  # Extra arguments passed to SAC, if any
        )

    def train(self, timesteps=100000, callback=None, checkpoint_freq=RL_Config.CHECKPOINT_FREQ):
        if callback is None:
            callback = default_callback(checkpoint_freq=checkpoint_freq, model_type="sac")
        self.model.learn(
            total_timesteps=timesteps,
            tb_log_name="SAC_" + datetime.now().strftime("%Y%m%d-%H%M%S"),
            callback=callback
        )

    def get_action(self, observation):
        action, _states = self.model.predict(observation, deterministic=self.deterministic)
        return action

    def save_model(self, path):
        path = adjust_save_path(path, model_type="sac")
        self.model.save(path)

    def load_model(self, path):
        path = adjust_load_path(path, model_type="sac")
        self.model = SAC.load(path, env=self.env)


--------------------------------------------------
File End
--------------------------------------------------


lander_learner/agents\__init__.py
File type: .py
from stable_baselines3.common.callbacks import CheckpointCallback
from datetime import datetime
from lander_learner.utils.config import RL_Config


def default_callback(checkpoint_freq=100000, checkpoint_dir=None, model_type="model"):
    """
    Create a default CheckpointCallback for training an RL agent.

    Args:
        timesteps (int): The number of timesteps between each checkpoint.
        checkpoint_dir (str): The directory to save checkpoints.

    Returns:
        CheckpointCallback: A callback that saves checkpoints every `timesteps` steps.
    """
    if checkpoint_dir is None:
        checkpoint_dir = RL_Config.DEFAULT_CHECKPOINT_DIR / f"{model_type}_{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    return CheckpointCallback(save_freq=checkpoint_freq, save_path=checkpoint_dir, name_prefix=model_type)


--------------------------------------------------
File End
--------------------------------------------------
